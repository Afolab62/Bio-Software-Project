{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a50139fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bc89b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLoader:\n",
    "    def load(self, filepath) -> pd.DataFrame:\n",
    "        if filepath.endswith(\".tsv\"):\n",
    "            return pd.read_csv(filepath, sep=\"\\t\")\n",
    "        elif filepath.endswith(\".json\"):\n",
    "            return pd.read_json(filepath)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported format\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed49aa6",
   "metadata": {},
   "source": [
    "### Column Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "735ce0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_fields = {\n",
    "    \"plasmid_variant_index\": {\"type\": str, \"nullable\": False},\n",
    "    \"parent_plasmid_variant\": {\"type\": str, \"nullable\": False},\n",
    "\n",
    "    \"directed_evolution_generation\": {\"type\": int, \"nullable\": False},\n",
    "\n",
    "    \"assembled_dna_sequence\": {\"type\": str, \"nullable\": False},\n",
    "\n",
    "    \"dna_quantification_fg\": {\"type\": float, \"nullable\": False},\n",
    "    \"protein_quantification_pg\": {\"type\": float, \"nullable\": False},\n",
    "\n",
    "    \"is_control\": {\"type\": bool, \"nullable\": False},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cf19026",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_synonyms = {\n",
    "\n",
    "    \"plasmid_variant_index\": [\"plasmid_variant_index\", \"variant_index\", \"plasmid_id\"],\n",
    "    \"parent_plasmid_variant\": [\"parent_variant\", \"parent_plasmid\", \"parent_id\"],\n",
    "\n",
    "    \"directed_evolution_generation\": [\"generation\", \"evolution_generation\", \"evo_gen\"],\n",
    "\n",
    "    \"assembled_dna_sequence\": [\"dna_sequence\", \"sequence\", \"assembled_sequence\"],\n",
    "\n",
    "    \"dna_quantification_fg\": [\"dna_concentration_fg\", \"dna_qty_fg\", \"dna_fg\"],\n",
    "    \"protein_quantification_pg\": [\"protein_concentration_pg\", \"protein_qty_pg\", \"protein_pg\"],\n",
    "\n",
    "    \"is_control\": [\"control\", \"is_control\", \"control_sample\"]\n",
    "}\n",
    "\n",
    "def clean_cols(col: str) -> str:\n",
    "    return col.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def build_synonym_map(col_synonyms): #Reverse synonym lookup\n",
    "    synonym_map = {}\n",
    "    for synonym, variants in col_synonyms.items():\n",
    "        for v in variants:\n",
    "            synonym_map[clean_cols(v)] = synonym\n",
    "    return synonym_map\n",
    "\n",
    "def validate_mapping(mapping): #Prevents uplicate assignments. Remove if confirmation can be require in fronten. \n",
    "    reverse = {}\n",
    "    for raw, field in mapping.items():\n",
    "        if field in reverse:\n",
    "            raise ValueError(\n",
    "                f\"Multiple columns mapped to '{field}': \"\n",
    "                f\"{reverse[field]} and {raw}\"\n",
    "            )\n",
    "        reverse[field] = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67c47ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_mapping_bulk(mapping):\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nProposed Column Mapping:\")\n",
    "        for raw, canon in mapping.items():\n",
    "            print(f\"{raw} → {canon}\")\n",
    "\n",
    "        response = input(\"\\nPress Enter to accept, or type 'edit' to modify: \").strip().lower()\n",
    "        if response == \"\":\n",
    "            break  # User accepted mapping\n",
    "\n",
    "        if response == \"edit\":\n",
    "            edits = input(\n",
    "                \"Enter edits as raw:target,raw2:target2,... : \"\n",
    "            ).strip()\n",
    "            for pair in edits.split(\",\"):\n",
    "                if \":\" not in pair:\n",
    "                    print(f\"Skipping invalid entry '{pair}'\")\n",
    "                    continue\n",
    "                raw_col, target_field = pair.split(\":\", 1)\n",
    "                raw_col = raw_col.strip()\n",
    "                target_field = target_field.strip()\n",
    "                if raw_col not in mapping:\n",
    "                    print(f\"Column '{raw_col}' not in proposed mapping. Skipping.\")\n",
    "                    continue\n",
    "                mapping[raw_col] = target_field\n",
    "            # After edits, loop prints updated mapping automatically\n",
    "        else:\n",
    "            print(\"Invalid input. Press Enter to accept or type 'edit' to modify.\")\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f458ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnMapper:\n",
    "    def __init__(self, essential_fields, col_synonyms):\n",
    "        self.essential_fields = list(essential_fields.keys())\n",
    "        self.synonym_map = build_synonym_map(col_synonyms)\n",
    "\n",
    "    def auto_map_by_synonym(self, columns):\n",
    "        mapping = {}\n",
    "        used_cols = set()\n",
    "\n",
    "        for col in columns:\n",
    "            if col in self.synonym_map:\n",
    "                official = self.synonym_map[col]\n",
    "                if official not in mapping.values():  # avoid duplicates\n",
    "                    mapping[col] = official\n",
    "                    used_cols.add(col)\n",
    "\n",
    "        return mapping, used_cols\n",
    "\n",
    "    def left_to_right_assign(self, columns, used_cols, existing_mapping): #If NOT already mapped\n",
    "\n",
    "        remaining_cols = [c for c in columns if c not in used_cols]\n",
    "        already_assigned_fields = set(existing_mapping.values())\n",
    "        remaining_fields = [f for f in self.essential_fields if f not in already_assigned_fields]\n",
    "\n",
    "        for col, field in zip(remaining_cols, remaining_fields):\n",
    "            existing_mapping[col] = field\n",
    "\n",
    "        return existing_mapping\n",
    "\n",
    "    def generate_mapping(self, df_columns):\n",
    "        # Track original ↔ cleaned names\n",
    "        original_to_clean = {c: clean_cols(c) for c in df_columns}\n",
    "        clean_to_original = {v: k for k, v in original_to_clean.items()}\n",
    "\n",
    "        cleaned_cols = list(clean_to_original.keys())\n",
    "\n",
    "        #Synonym mapping\n",
    "        mapping, used = self.auto_map_by_synonym(cleaned_cols)\n",
    "\n",
    "        #Left-to-right\n",
    "        mapping = self.left_to_right_assign(cleaned_cols, used, mapping)\n",
    "\n",
    "        #Validate before user sees it\n",
    "        validate_mapping(mapping)\n",
    "\n",
    "        mapped_fields = set(mapping.values())\n",
    "        missing_fields = [f for f in self.essential_fields if f not in mapped_fields]\n",
    "\n",
    "        if missing_fields:\n",
    "            print(\"\\n⚠️ Missing essential fields (not found in file):\", missing_fields)\n",
    "\n",
    "\n",
    "        #User confirmation - do in front en later?\n",
    "        mapping = confirm_mapping_bulk(mapping)\n",
    "\n",
    "        # Convert cleaned names back to original DataFrame column names\n",
    "        final_mapping = {clean_to_original[k]: v for k, v in mapping.items()}\n",
    "        return final_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aea443",
   "metadata": {},
   "source": [
    "### QC / Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3460b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coerce_types(df, essential_fields):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col, dtype in essential_fields.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Handle numeric types safely\n",
    "        if dtype in [int, float]:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        # Strings\n",
    "        elif dtype == str:\n",
    "            df[col] = df[col].astype(str)\n",
    "        # Boolean / control fields\n",
    "        elif dtype == bool:\n",
    "            df[col] = df[col].map(lambda x: bool(x) if pd.notna(x) else pd.NA)\n",
    "        else:\n",
    "            # fallback: keep original\n",
    "            df[col] = df[col]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "class QCValidator:\n",
    "    def __init__(self, essential_fields):\n",
    "        self.essential_fields = essential_fields\n",
    "\n",
    "    def validate(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        error_records = {}   # row_index → error string\n",
    "        valid_mask = []      # True = valid row, False = rejected\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            errors = []\n",
    "\n",
    "            # --- Required field presence ---\n",
    "            for field in self.essential_fields:\n",
    "                if pd.isna(row.get(field)):\n",
    "                    errors.append(f\"Missing value for {field}\")\n",
    "\n",
    "            # --- Logical / biological rules (NA-safe) ---\n",
    "            gen = row.get(\"directed_evolution_generation\")\n",
    "            if pd.notna(gen) and gen < 0:\n",
    "                errors.append(\"Generation cannot be negative\")\n",
    "\n",
    "            dna_q = row.get(\"dna_quantification_fg\")\n",
    "            if pd.notna(dna_q) and dna_q < 0:\n",
    "                errors.append(\"DNA quantification cannot be negative\")\n",
    "\n",
    "            prot_q = row.get(\"protein_quantification_pg\")\n",
    "            if pd.notna(prot_q) and prot_q < 0:\n",
    "                errors.append(\"Protein quantification cannot be negative\")\n",
    "\n",
    "            seq = row.get(\"assembled_dna_sequence\")\n",
    "            if pd.notna(seq):\n",
    "                seq = str(seq).upper()\n",
    "                if not set(seq).issubset({\"A\", \"T\", \"C\", \"G\", \"N\", \"R\", \"Y\"}): #Included ambiguity codes for now\n",
    "                    errors.append(\"DNA sequence contains invalid characters\")\n",
    "            \n",
    "\n",
    "            # --- Record result ---\n",
    "            if errors:\n",
    "                error_records[idx] = \"; \".join(errors)\n",
    "                valid_mask.append(False)\n",
    "            else:\n",
    "                valid_mask.append(True)\n",
    "\n",
    "        # Convert mask to pandas Series aligned with df index\n",
    "        mask_series = pd.Series(valid_mask, index=df.index)\n",
    "\n",
    "        valid_df = df[mask_series].copy()\n",
    "        rejected_df = df[~mask_series].copy()\n",
    "\n",
    "        if not rejected_df.empty:\n",
    "            rejected_df[\"qc_error_reason\"] = rejected_df.index.map(error_records)\n",
    "\n",
    "        return valid_df, rejected_df\n",
    "    \n",
    "def summarize_qc_errors(rejected_df):\n",
    "    \n",
    "    if rejected_df.empty:\n",
    "        print(\"No QC errors!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nQC Summary:\")\n",
    "\n",
    "    # Iterate over unique error reasons\n",
    "    error_groups = rejected_df.groupby(\"qc_error_reason\")\n",
    "\n",
    "    for reason, group in error_groups:\n",
    "        # Convert index to 1-based row numbers\n",
    "        rows = (group.index + 1).tolist()\n",
    "        count = len(rows)\n",
    "        print(f\"- {reason} (Count: {count}, Rows: {rows})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d29afe",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "24d61ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 301 rows\n",
      "Original columns: ['Plasmid_Variant_Index', 'Parent_Plasmid_Variant', 'Directed_Evolution_Generation', 'Assembled_DNA_Sequence', 'DNA_Quantification_fg', 'Protein_Quantification_pg', 'Control']\n",
      "\n",
      "Proposed Column Mapping:\n",
      "plasmid_variant_index → plasmid_variant_index\n",
      "control → is_control\n",
      "parent_plasmid_variant → parent_plasmid_variant\n",
      "directed_evolution_generation → directed_evolution_generation\n",
      "assembled_dna_sequence → assembled_dna_sequence\n",
      "dna_quantification_fg → dna_quantification_fg\n",
      "protein_quantification_pg → protein_quantification_pg\n",
      "Valid rows: 301\n",
      "Rejected rows: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"c://Users//Leora//OneDrive - Queen Mary, University of London//Group_Project//Example_Data//DE_BSU_Pol_Batch_1.tsv\"\n",
    "\n",
    "loader = FileLoader()\n",
    "df = loader.load(file_path)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "\n",
    "mapper = ColumnMapper(essential_fields, col_synonyms)\n",
    "column_mapping = mapper.generate_mapping(df.columns)\n",
    "\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Ensure all essential columns exist\n",
    "for col in essential_fields:\n",
    "    if col not in df.columns:\n",
    "        missing_fields = [f for f in essential_fields if f not in df.columns]\n",
    "        raise ValueError(\n",
    "        f\"⚠️ Missing essential fields in file: {missing_fields}. \"\n",
    "        \"Ingestion cannot continue until these columns are present.\"\n",
    "    )\n",
    "\n",
    "\n",
    "df = coerce_types(df, essential_fields)\n",
    "#print(df.dtypes)\n",
    "#print(df.head())\n",
    "\n",
    "validator = QCValidator(essential_fields)\n",
    "valid_df, rejected_df = validator.validate(df)\n",
    "\n",
    "print(\"Valid rows:\", len(valid_df))\n",
    "print(\"Rejected rows:\", len(rejected_df))\n",
    "\n",
    "if len(rejected_df) > 0:\n",
    "    summarize_qc_errors(rejected_df)\n",
    "    raise ValueError(\"QC FAILED — Fix file before ingestion.\")\n",
    "\n",
    "\n",
    "input(\"Press Enter to continue.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15105b",
   "metadata": {},
   "source": [
    "SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd24d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
